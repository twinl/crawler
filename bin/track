#!/bin/bash
# hourly: run hourly process of collecting data from Twitter
# usage: hourly (from crontab)
# 20101216 erikt(at)xs4all.nl

EXP="`basename $0`"
BASEDIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )"/.. >/dev/null 2>&1 && pwd )"/
DATADIR="$BASEDIR$EXP/"
LOGDIR="${BASEDIR}etc/"
DATE="`date '+%Y%m%d-%H'`"
MIN="`date '+%M'`"
OUTFILE="$DATE.out"
LANG=en_US.UTF-8
export LANG

function start_crawler {
   nohup stdbuf -oL -eL $BASEDIR/bin/makeoauth.$EXP \
                        2>>${LOGDIR}errorlog </dev/null |\
                        ${BASEDIR}/bin/save_output.py $DATADIR
}

function stop_crawler {
   ps -ef | grep curl | grep ${EXPWORD} |\
      tr -s ' ' ' ' | sed 's/^ *//' | cut -d' ' -f2 | xargs kill -9 2>/dev/null
}

function test_crawler_is_running {
   ps -ef | grep curl | grep ${EXPWORD} | grep -v grep
}

function cleanup_output_files {
   cd $DATADIR
   for FILE in *.out
   do
      if [ "$FILE" != "$OUTFILE" ]
      then
         chmod 444 $FILE
         gzip $FILE &
      fi
   done
}

function get_longest_query_word {
   TMPFILE="/tmp/$EXP.$$.$RANDOM"
   cat $LOGDIR$EXP | tr , '\n' | grep -v = > $TMPFILE
   WORD="`sed 's/./1/g' $TMPFILE | paste -d' ' - $TMPFILE | sort -n | tail -1 | cut -d' ' -f2`"
   rm -f $TMPFILE
   echo $WORD
}

EXPWORD="`get_longest_query_word`"

if [ "$MIN" = "00" ]
then
   cleanup_output_files &
fi
if [ -z "`test_crawler_is_running`" ]
then
   start_crawler
   sleep 1
   $0 &
fi

exit 0
