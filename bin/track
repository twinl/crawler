#!/bin/bash
# hourly: run hourly process of collecting data from Twitter
# usage: hourly (from crontab)
# 20101216 erikt(at)xs4all.nl

EXP="`basename $0`"
BASEDIR="/home/cloud/"
DATADIR="$BASEDIR$EXP/"
LOGDIR="${BASEDIR}etc/"
DATE="`date '+%Y%m%d-%H'`"
MIN="`date '+%M'`"
OUTFILE="$DATE.out"
LANG=en_US.UTF-8
export LANG

function start_crawler {
   nohup stdbuf -oL -eL $BASEDIR/bin/makeoauth.$EXP >> $DATADIR$OUTFILE \
                                         2>>${LOGDIR}errorlog </dev/null &
}

function stop_crawler {
   ps -ef | grep curl | grep ${EXPWORD} |\
      tr -s ' ' ' ' | cut -d' ' -f2 | xargs kill -9 2>/dev/null
}

function test_crawler_is_running {
   ps -ef | grep curl | grep ${EXPWORD} | grep -v grep
}

function cleanup_output_files {
   cd $DATADIR
   for FILE in *.out
   do
      if [ "$FILE" != "$OUTFILE" ]
      then
         chmod 444 $FILE
         gzip $FILE &
      fi
   done
}

function get_longest_query_word {
   TMPFILE="/tmp/$EXP.$$.$RANDOM"
   cat $LOGDIR$EXP | tr , '\n' | grep -v = > $TMPFILE 
   WORD="`sed 's/./1/g' $TMPFILE | paste -d' ' - $TMPFILE | sort -n | tail -1 | cut -d' ' -f2`"
   rm -f $TMPFILE
   echo $WORD
}

EXPWORD="`get_longest_query_word`"

if [ "$MIN" = "00" ]
then
   stop_crawler
   while [ -n "`test_crawler_is_running`" ]
   do
      sleep 1
   done
   start_crawler
   cleanup_output_files
else
   if [ -z "`test_crawler_is_running`" ]
   then
      start_crawler
   fi
fi

exit 0
